# 快速部署机器学习模型

本文面向数据分析师和算法工程师，尝试解决这个几乎必然遇到的问题： 当我训练好了一个模型，怎么部署给IT工程师使用？

当然还必须具备的是，部署应该足够简单，且无需修改模型本身的代码，就像工厂生产了一个产品，只需要在外部套上一个包装盒就能快递给别人了，并不需要修改该产品本身。


## 1. 当我训练好了一个模型，怎么部署给IT工程师使用？

我们训练好一个模型之后，都会涉及到怎么交付的问题，其他人（如IT工程师）应该怎么才能使用该模型呢？例如我们训练好的分类模型如下：

```python
def classify(data):
    # 加载模型
    model = load_model()

    # 对数据进行预测
    res = model.predict(data)

    # 返回预测结果
    return res
```

上面就是一个分析师或者算法工程师常用的代码了，但是这应该怎么给IT工程师使用呢？一种比较原始的方式是，直接将这些代码给他，把模型文件也给他，告诉他需要注意什么什么。

这种方式看似简单，但是却埋下很大的坑，类似一种紧耦合的关系，依赖与调试也很麻烦。如果算法工程师的工程水平比较好，可能可以做成安装包，使用`python setup.py install`这样的形式来安装，能避免一部分问题，不过这对算法工程师的要求有点高，而且这也还有依赖关系的问题。

## 2. 我们的解决方案

我们所追求的应该是，算法工程师开发好一个模型，其他人直接调用就可以了，不应该存在依赖问题。把接口封装成http接口就能解决这个问题，只是传统的开发http接口，对算法工程师有点不够友好，所以我们封装了一个适合算法工程师使用的包`https://github.com/ibbd-dev/python-fire-rest`。直接按readme文件上的介绍，很容易安装。

使用上很简单：

```python
from fireRest import API, app

# 包装classify成http服务
API(classify)

app.run(port=5000)
```

就三行代码就启动了一个http接口服务，完全非侵入式（不需要改classify），算法工程师还是专注于算法实现即可。算法工程师在完善模型的时候，也完全不需要考虑其他人的使用问题了，只有输入输出变更的时候，才需要告知。

其他人如果需要使用该模型，这时他们不需要知道模型的内部细节，也没有依赖，只需要知道模型的输入输出，就可以访问其http接口：

```sh
curl -XPOST localhost:5000/classify -d '[[1,1,2,1,4]]'
```

这是在命令行下使用curl进行请求的，在Python里只要使用`requests`包即可，也很简单。该接口的返回值也很简单：

```json
{
    "code": 0,
    "data": "这里是接口的返回内容，可以是字符串，整数，列表，字典等格式。",
    "messages": null
}
```

其中的data就是模型函数classify的返回值。

## 3. 让别人可以访问

上面的方式启动接口服务之后，只能在自己的电脑上访问，别的同事还是访问不到。如果需要公司内的同事（局域网内）能够访问到，需要做一点改变：

```python
from fireRest import API, app

# 包装classify成http服务
API(classify)

# 加上host参数，这样局域网内的同事就能通过你的内网ip进行访问了
app.run(port=5000, host='0.0.0.0')
```

说明：关于内网ip可以在网络设置中查到，通常是`192.168.*.*`。

关于这个包更多的使用方式，[可以看这里](https://github.com/ibbd-dev/python-fire-rest)。

## 4. 进阶实践

1. 把加载模型放到全局加载，这样在访问接口的时候，就不需要再加载一次，加快接口的访问。
2. 将接口服务封装到docker镜像中，这样交付就更加方便，别使用的时候，只需要启动即可，不用管烦人的依赖问题。
3. 在接口中对用户输入的参数进行检验，如果发现异常及时用`raise`抛出异常。

## 总结

说白了，其实就是一句：使用http接口进行解耦合。我们开源的这个包只是让这个工作更加简单而已。

