# ETL数据清洗流程

## 数据清洗需要达到的目标

- 数据完整性：例如如果我们需要按星期几进行分析，但是却没有该字段，但是有时间字段
- 数据唯一性：数据中是否有需要去重的数据，去重的标准是什么
- 数据权威性：同一指标可能有多个来源，而且值还不一样，这时怎么规范该指标。
- 数据合法性：数据中出现了不该出现的值，例如部门字段中出现了“跟”这样的值，那这个值是否是合法的，出现的原因是什么。
- 数据一致性：同一个部门字段中，例如“公安局”，有些记录可能是“公安局”，有些记录可能是“市公安局”等，这些需要做规范。


## 数据清洗的步骤

### STEP0 理解业务背景及数据逻辑
这是做分析与清洗的基础。关键问题：

- 业务背景是什么？
- 业务流程与逻辑是什么？
- 数据是怎么产生的？
- 客户需要达到的目标是什么？

问对了问题，你就成功了一半。

### STEP1 导入原始数据到Insight
原始数据通常在客户的数据库中，第一步要做的就是先将它们的原始数据导入到Insight中，作为数据探索的基础数据。以后做数据清洗通常也是基于Insight中的数据去做，避免每次都要操作客户的数据库（客户的数据库可能是乱七八糟的，设计可能也不合理，索引等等也没有）。

这一步通常会做简单的数据清洗，例如整理成宽表，格式化数据，定义字段类型，索引等。

这步也有一些常用的处理经验：

- 对于类似title，name，content等类似的字段，都是需要分析的。
- 对于需要做词云的字段，IK分词通常比较难达到效果，可以再外部使用结巴分词来提取关键词。
- 建立数据集的时候，需要指定主时间字段，特别是有多个时间字段时，应该选择一个合适的作为主时间字段。
- 对于主时间字段，Insight可以进行自由的细分，不过如果有按星期几或者按小时段进行统计的需求，就需要增加相应的字段。
- 对于有多个时间字段的，往往需要统计他们的时间差（单位可以看具体需求，选择天或者小时等），例如需要统计相应时间的分布等，这时可以增加相应的字段。

### STEP2 数据探索
基于已有数据集进行数据探索（可视化探索）。

我们知道了业务背景，流程与问题，但是问题的症结在哪里，这个就需要不断地进行探索才能发现。例如，客户说销售最近不好，这是一个问题，那在数据上我们至少可以有以下问题：

- 这个不好在数据上是怎么体现的？
- 不好是什么时候不好，那些渠道不好？
- 等等

数据探索的另一方面就是文本分类，例如一大堆文本需要做分类，这时通常是还没有一个好样本的，首先需要做的就是整理一个好的样本，具体可见[这里](https://github.com/IBBD/IBBD.github.io/blob/master/tech/text-classify.md)。

### STEP3 数据清洗
数据探索会得到一些可视化的表现形式，但是有一些重要的形式可能根据目前的数据集无法完成，这时就需要做数据清洗了，清洗之后重新导入。

清洗的方式通常有：增减字段，增纬、减纬、行列转换、拆分表、合并表、统一类型（格式转换）、字段日期和数字计算及转换、列数据拆分为行数据、行数据合并为列数据。

STEP2到STEP3需要不断的重复和优化的。

### STEP4 数据分析
数据分析的三个核心思想：

- 趋势
- 对比
- 细分


